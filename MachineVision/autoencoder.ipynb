{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1LWk1t4e8-M6cCe5qLWoZ1Q2eSbXnqddY",
      "authorship_tag": "ABX9TyMQd2z+MnTYAxnNaEdXaTyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niyanchun/AI-Learning/blob/master/MachineVision/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gd-W0JQDwb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08644826-b7b4-4e4a-a18b-7480a0e7379f"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "import cv2\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "# from pyimagesearch.conv_autoencoder import ConvAutoencoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose,\n",
        "                                     LeakyReLU, Activation, Flatten,\n",
        "                                     Dense, Reshape, Input)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "\n",
        "class ConvAutoencoder:\n",
        "\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # define the input to the encoder\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = inputs\n",
        "\n",
        "        for f in filters:\n",
        "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # flatten the network and then construct our latent vector\n",
        "        volumeSize = K.int_shape(x)\n",
        "        x = Flatten()(x)\n",
        "        latent = Dense(latentDim)(x)\n",
        "\n",
        "        encoder = Model(inputs, latent, name=\"encoder\")\n",
        "        print(encoder.summary())\n",
        "\n",
        "        latentInputs = Input(shape=(latentDim,))\n",
        "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\n",
        "        for f in filters[::-1]:\n",
        "            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "        outputs = Activation(\"sigmoid\")(x)\n",
        "\n",
        "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
        "\n",
        "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
        "\n",
        "        return encoder, decoder, autoencoder\n",
        "\n",
        "\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-s\", \"--samples\", type=int, default=8,\n",
        "#                 help=\"# number of samples to visualize when decoding\")\n",
        "# ap.add_argument(\"-o\", \"--output\", type=str, default=\"output.png\",\n",
        "#                 help=\"path to output visualization file\")\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "#                 help=\"path to output plot file\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "args = {}\n",
        "args[\"samples\"] = 8\n",
        "args[\"output\"] = \"output.png\"\n",
        "args[\"plot\"] = \"plot.png\"\n",
        "\n",
        "Epoch = 25\n",
        "BatchSize = 32\n",
        "\n",
        "print(\"loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "print(\"building autoencoder...\")\n",
        "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
        "opt = Adam(lr=1e-3)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "\n",
        "H = autoencoder.fit(trainX, trainX, validation_data=(testX, testX), epochs=Epoch, batch_size=BatchSize)\n",
        "\n",
        "N = np.arange(0, Epoch)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "# loss: 训练集上的loss， val_loss: 测试集上的loss\n",
        "# loss一直下降、收敛，val_loss却上升、不收敛，说明过拟合了\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "# plt.show()\n",
        "plt.savefig(args[\"plot\"])\n",
        "\n",
        "print(\"making predictions...\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "outputs = None\n",
        "\n",
        "for i in range(0, args[\"samples\"]):\n",
        "    original = (testX[i] * 255).astype(\"uint8\")\n",
        "    recon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\n",
        "    output = np.hstack([original, recon])\n",
        "    if outputs is None:\n",
        "        outputs = output\n",
        "\n",
        "    else:\n",
        "        outputs = np.vstack([outputs, output])\n",
        "\n",
        "cv2.imwrite(args[\"output\"], outputs)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n",
            "loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "building autoencoder...\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                50192     \n",
            "=================================================================\n",
            "Total params: 69,392\n",
            "Trainable params: 69,200\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "60000/60000 [==============================] - 19s 314us/sample - loss: 0.0188 - val_loss: 0.0115\n",
            "Epoch 2/25\n",
            "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0103 - val_loss: 0.0096\n",
            "Epoch 3/25\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.0093 - val_loss: 0.0088\n",
            "Epoch 4/25\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.0087 - val_loss: 0.0085\n",
            "Epoch 5/25\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0083 - val_loss: 0.0084\n",
            "Epoch 6/25\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0080 - val_loss: 0.0080\n",
            "Epoch 7/25\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0078 - val_loss: 0.0078\n",
            "Epoch 8/25\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0076 - val_loss: 0.0075\n",
            "Epoch 9/25\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0074 - val_loss: 0.0075\n",
            "Epoch 10/25\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0073 - val_loss: 0.0072\n",
            "Epoch 11/25\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0072 - val_loss: 0.0072\n",
            "Epoch 12/25\n",
            "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0071 - val_loss: 0.0071\n",
            "Epoch 13/25\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0070 - val_loss: 0.0071\n",
            "Epoch 14/25\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0070 - val_loss: 0.0075\n",
            "Epoch 15/25\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 16/25\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0068 - val_loss: 0.0069\n",
            "Epoch 17/25\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0068 - val_loss: 0.0069\n",
            "Epoch 18/25\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0067 - val_loss: 0.0068\n",
            "Epoch 19/25\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0067 - val_loss: 0.0069\n",
            "Epoch 20/25\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0066 - val_loss: 0.0069\n",
            "Epoch 21/25\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0066 - val_loss: 0.0069\n",
            "Epoch 22/25\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0066 - val_loss: 0.0070\n",
            "Epoch 23/25\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0065 - val_loss: 0.0068\n",
            "Epoch 24/25\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0065 - val_loss: 0.0067\n",
            "Epoch 25/25\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0065 - val_loss: 0.0068\n",
            "making predictions...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}